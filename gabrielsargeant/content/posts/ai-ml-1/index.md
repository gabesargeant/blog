---
title: "My thoughts about the future of my work and how AI will impact me alone. Yes, that's correct I don't care about how it impacts other people. Let the world burn..."
date: 2023-02-15
draft: false
---

{{< note pos="right" txt="This is actually a how to guide on what I think is best practice with OpenAI at the moment. YMMV">}}

I was watching a Tom Scott video about AI, more precisely ChatGPT. [Linked Here](https://www.youtube.com/watch?v=jPhJbKBuNnA)  He was being his usual amazed self and both marveling and worrying about at how good it was at coding. So, the next day at work I started using OpenAI's chat bot to help out with work. It was kinda good. I'm not hating this future.

{{< note pos="left" txt="At the end of the video, Mr Scott remarks that he doesn't want AI trained on his video essays. I feel that Mr Scott is missing an  opportunity where he could catfish more views out of his audience as he wraps up his youtube career. *I maybe gone, but a facsimile of me remains, I feed it a copy of wired each week and it makes a video*. As all millennials know you gotta hustle for that passive income :D" >}}

After watching Tom Scott's video. My next workday started like this:

- 8:30 - Start describing a problem I have at work to ChatGPT and playing with the results in a Jupyter Notebook. 
- 10:00 - Amazed at the stacks of code the AI was generating I went for a coffee and espoused my amazement at just how good ChatGPT was to everyone I could. 
- 10:30 - Starting to shake out the main logic of the code I realize, nothing the AI has written comes close to touching the complexity of the problem I'm dealing with. 
- 11:00 - I'm back to writing code, but using Chat GPT in a different way. And this is where things got really productive for me. 

It took me half a day of using AI to figure out that I'm going to be the one doing the software construction. At least until either I die, or rage quit. Computer's are not taking our jobs in that regard. However, If I were running Stack Overflow I'd be scared to death. 

At it's core, ChatGPT read the docs. It read all of them, it knows it all. **All of the docs**. When you ask it a question it gives you an expert answer. That's how it'll change work. Less fuck-ups. And over time, more common code. 

{{< img src="rtfm.png" alt="ChatGPT read the fucking manual">}} 

It's fun to ponder if the AI is consistent in it's answers. There's a million ways to describe a problem. I imagine asking Java questions to the AI will give you lots of answers in the many fad like flavors that have graced Java over the years. Something like golang maybe a bit consistent because it's a more centrally orthodox and opinionated language. This is something to keep looping over to as the tech progresses.

So I think AI will eventually own all the boilerplate work. And I'm really happy to cede it this ground.

However, GitHub Co-pilot and other helper intellisense coding programs seem less useful to me. I've always thought about the process of writing software as describing what the real problem is. Helper software like co-pilot give you boosts in snippets, but you still need to know where you're going and what you want to get done. It's the same with ChatGPT. It requires direction. 

TAs I've been using ChatGPt the flow I found useful was asking for help with the tedious bits. A good example was doing some file system manipulation. It's not super hard, but I don't do it often enough for it to be second nature to me. I always end up going back to the docs to reread. When this happens I always feel like I'm taking way too much time on the small stuff. I would spend 10-45 minutes stuffing about in the docs and now it's down to 30 seconds where I type out what I need to do and AI hammers out a quick solution I can start with. This will be the most significant use of Chat GPT IMO.







